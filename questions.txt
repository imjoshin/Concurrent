<w>
	<q>01-Intro.pdf
	<q>02-Hardware-OS.pdf

<w>
	<q>02-Hardware-OS.pdf
	<q>03-Process.pdf

	<q>What is a CPU mode?
	<a> CPU modes (also called processor modes, CPU states, CPU privilege levels and other names) are operating modes for the central processing unit of some computer architectures that place restrictions on the type and scope of operations that can be performed by certain processes being run by the CPU. This design allows the operating system to run with more privileges than application software. Ideally, only highly trusted kernel code is allowed to execute in the unrestricted mode; everything else (including non-supervisory portions of the operating system) runs in a restricted mode and must use a system call to request the kernel perform on its behalf any operation that could damage or compromise the system, making it impossible for untrusted programs to alter or damage other programs (or the computing system itself).

	<q>What is dual-mode execution?
	<a> Modern CPUs have two execution modes: the <b>user mode</b> and the <b>supervisor</b> (or system, kernel, privileged) mode, controlled by a mode bit.

	<q>What is a privileged instruction? Why are privileged instructions needed?
	<a> A machine code instruction that may only be executed when the processor is running in supervisor mode. <b>Privileged instructions</b> include operations such as I/O and memory management. 

	<q>What is an atomic instruction? Why are atomic instructions needed? What would happen if multiple CPUs/cores execute their atomic instructions?
	<a> These instructions execute as one uninterruptible unit. More precisely, when such an instruction is run, all other instructions being executed in various stages by the CPUs will be stopped (and perhaps re-issued later) until this instruction finishes. If two such instructions are issued at the same time, even though on different CPUs or cores, they will be executed sequentially.


	<q>What is an interrupt, and what is a trap?
	<a> An event that requires the attention of the OS is an <b>interrupt</b>. These events include the completion of an I/O, a keypress, a request for service, a division by zero and so on. Interrupts may be generated by hardware or software. An interrupt generated by software (i.e., division by 0) is usually referred to as a <b>trap</b>.

	<q>Which one of the following event is an interrupt/trap? Why?
	<a>
	<ul>
		<li>
			<b>Real time clock goes off</b> - Interupt
		</li>
		<li>
			<b>A keypress</b> - Interupt
		</li>
		<li>
			<b>A segment fault</b> - Trap
		</li>
		<li>
			<b>A modem dial-up call</b> - Interupt
		</li>
		<li>
			<b>Floating-point exception</b> - Trap
		</li>
		<li>
			<b>Accessing an area not belonging to your program</b> - Trap
		</li>
		<li>
			<b>The completion of an I/O</b> - Interupt
		</li>
		<li>
			<b>In virtual memory accessing a page that is not in the physical memory</b> - Interupt
		</li>
		<li>
			<b>A system call</b> - Trap
		</li>
		<li>
			<b>An memory parity error</b> - Trap
		</li>
	</ul>

	<q>What does it mean by interrupt-driven?<br>
	<a> Modern operating systems are interrupt driven, meaning the OS is in action only if an interrupt occurs.

	<q>Explain the steps the hardware and the operating system will do when an interrupt occurs.
	<a> The OS is activated by an interrupt. The executing program is suspended. Control is transferred to the OS. A program will be resumed when the service completes.

	<q>What is a system call?
	<a> System calls provide an interface to the services made available by an operating system. A system call generates an interrupt (actually a trap), and the caller is suspended.
	<br><br>Types of system calls:'
	<ul>
		<li>
		<b>Process control</b> (e.g., create and destroy processes)
		</li>
		<li>
		<b>File management</b> (e.g., open and close files)
		</li>
		<li>
		<b>Device management</b> (e.g., read and write operations)
		</li>
		<li>
		<b>Information maintenance</b> (e.g., get time or date)
		</li>
		<li>
		<b>Communication</b> (e.g., send and receive messages)
		</li>
	</ul>

	<q>Why is an interval timer needed?
	<a> Because the operating system must maintain the control over the CPU, it has to prevent a user program from getting the CPU forever without calling for system service (i.e., I/O). Before a user program runs, the OS sets the interval timer to certain value. Once the interval timer counts down to 0, an interrupt is generated and the OS can take appropriate action.

	<q>What is a process?
	<a> A process is a program in execution.

	<q>How process space is allocated?
	<a> A process is more than a program, because a process has a program counter, stack, data section, code section, etc (i.e., runtime information). If the stack and heap meet, the process is out of memory.

	<q>What are process states?
	<a> At any moment, a process can be in one of the five states:<br>
	<ul>
		<li>
			<b>New:</b> The process is being created
		</li>
		<li>
			<b>Running:</b> The process is executing on a CPU
		</li>
		<li>
			<b>Waiting:</b> The process is waiting for some event to occur (e.g., waiting for I/O completion)
		</li>
		<li>
			<b>Ready:</b> The process has everything but the CPU. It is waiting to be assigned to a processor.
		</li>
		<li>
			<b>Terminated:</b> The process has finished execution
		</li>
	</ul>

<w>
	<q>03-Process.pdf

	<q>What is a PCB? What information should be stored in a PCB?
	<a> A <a href='pcb.png' target='_blank'><b>Process Control Block</b></a> stores a process's pointer, process state, process ID, program counter, registers, scheduling info, memory limits, and list of open files.

	<q>Can a process assume the system scheduling policy in order to run properly? Why?
	<a> When a CPU is free, the CPU scheduler looks at the ready queue, picks a process, and resumes it. The way of picking a process from the ready queue is referred to as scheduling policy. We cannot make any assumptions about it.

	<q>What is a CPU scheduler?
	<a> When a CPU is free, the CPU scheduler looks at the ready queue, picks a process, and resumes it.

	<q>What is the context of a process?
	<a> The context of a process includes process ID, process state, the values of CPU registers, the program counter, and other memory/file management information (i.e., execution environment).

	<q>What is a context switch? Show all needed steps in a context switch.
	<a> After the CPU scheduler selects a process and before allocates CPU to it, the CPU scheduler must save the context of the currently running process, put it back to the ready queue, load the context of the selected process, and go to the saved program counter.

	<q>Understand the following system calls 
	<a href='http://linux.die.net/man/2/fork' target='_blank'>fork()</a>, 
	<a href='http://linux.die.net/man/2/wait' target='_blank'>wait()</a>, and 
	<a href='http://linux.die.net/man/3/execvp' target='_blank'>execvp()</a>.
	<a>

	<q>Understand the following shared memory related functions and system calls 
	<a href='http://linux.die.net/man/3/ftok' target='_blank'>ftok()</a>, 
	<a href='http://linux.die.net/man/2/shmget' target='_blank'>shmget()</a>, 
	<a href='http://linux.die.net/man/2/shmat' target='_blank'>shmat()</a>, 
	<a href='http://linux.die.net/man/2/shmdt' target='_blank'>shmdt()</a>, and 
	<a href='http://linux.die.net/man/2/shmctl' target='_blank'>shmctl()</a>.
	<a> <a href='http://www.cs.cf.ac.uk/Dave/C/node27.html' target='_blank'>Shared Memory example</a>


<w>
	<q>04-Thread.pdf

	<q>What is a thread or lightweight process?
	<a> A basic unit of CPU execution, and is created by a process.
	<q>What are the resources needed to run a thread?
	<a> A thread has a thread ID, a program counter, a register set and a stack. It is similar to a process,
		however, a thread shares with other threads in the same process, its code section, data section
		and other OS resources (files and signals) 
	<q>Why are threads cheaper than processes?
	<a> threads share the same resources of the main process
	<q>What are the major benefits of using threads?
	<a> 
			<ul>
				<li>
					<b>Responsiveness</b>: other parts of a program <i>(ex: other threads)</i> may still run even if one part <i>(ex: another threads)</i> is blocked
				</li>
				<li>
					<b>Resource Sharing</b>: threads of a process, by default, share many system resources <i>(ex: files and memory)</i>
				</li>
				<li>
					<b>Economy</b>: creating and terminating processes, allocating memroy and resources and context switching processes are very time consuming
				</li>
				<li>
					<b>Utilization of Multiprocessor Architecture</b>: multiple CPUs may run multiple threads of the same process. No program change is necessary
				</li>
			</ul>
	<q>What is a user-level thread?
	<a> User threads are supported at the <b>user level</b> and the kernel is not aware of user threads. A library provides all support for thread creating, termination, joining, and scheduling.
	<q>What is a kernel supported thread?
	<a> Kernel threads are supported by the kernel. The kernel does thread creation, termination, joing and scheduling in the kernel space. 
	<q>Describe the major advantages and disadvantages of using user-level threads and kernel supported threads.
	<a>
			<ul>
				<li>
					<b>Kernel threads</b> are usually <b>slower</b> than user threads due to system overhead. However, blocking one thread will not cause other threads of the same process to block, the kernel simply runs other kernel threads. In a multiprocessor environment, the kernel may schedule threads on different processors.
				</li>
				<li>
					Since there is no kernel intervention, <b>user threads</b> are usually more efficient. However, if one thread is blocked, all threads of the same process are also blocked, because the containing process is blocked. This is because the kernel only recognizes the containing process (of the threads)
				</li>
			</ul>
	<q>What are the thread models?
	<a>
			different systems support threads in different ways. 3 common thread models:
			<ul>
				<li>
					<b>many-to-one</b>
				</li>
				<li>
					<b>one-to-one</b>
				</li>
				<li>
					<b>many-to-many</b>
				</li>
			</ul>
	<q>How threads of a process are scheduled in each model?
	<a> 
			<ul>
				<li>
					<b>many-to-one</b>: one kernel threads (or process) has multiple user threads, user thread model
				</li>
				<li>
					<b>one-to-one</b>: one user thread maps to one kernel thread <i>(e.g. old Unix/Linux and Windows systems)</i>
				</li>
				<li>
					<b>many-to-many</b>: multiple user threads map to a number of kernel threads
				</li>
			</ul>
	<q>Understand the six points discussed in the multicore programming section.
	<a> With a single-core CPU threads are scheduled by a scheduler and can only run one at a time. With a multicore CPU multiple threads may run at the same time, one on each core. There are 5 main issues:
			<ul>
				<li>
					<b>Dividing Activies</b>: since each thread can run on a core, one must study the problem in hand so that program activites can be divided and run concurrently <i>(e.g. matrix multiplication)</i>
				</li>
				<li>
					<b>Balance</b>: make sure that each threads has <b>equal</b> contribution (if possible) to the whole computation
				</li>
				<li>
					<b>Data Splitting</b>: data may also be split into different section so that each can be separately processed <i>(e.g. matrix multiplication and quicksort after partitioning)</i>
				</li>
				<li>
					<b>Data Dependency</b>: watch for data items that are used by different threads. <i>(e.g. two threads may update a common variable at the same time)</i>. This can cause unexpected results and thus requires <b>synchronization</b> so only one thread can update a shared variable
				</li>
				<li>
					<b>Testing and Debugging</b>: threaded programming is <b>dynamic</b> causing unpredictable bugs. Some debugging issues do not have efficient solutions <i>(e.g. race conditions and system deadlock)</i>
				</li>
			</ul>
	<q>What are the two commonly used thread cancellation methods?
	<a> thread cancellation means terminating a thread before its completion. The thread to be cancelled is the <b>target thread</b> the two types of cancellation:
			<ul>
				<li>
					<b>Asynchronous</b>: the target threads terminates immediately. If the target thread owns some system-wide resource the system may not be able to reclaim these resources because other threads may be using them
				</li>
				<li>
					<b>Deferred</b>: the target thread periodically checks if it should terminate, allowing termination to be done in a controlled manner. The point the thread terminates itself is refferred to as the <b>cancellation point</b>. Reclaiming system-wide resources is not a problem since the threads determines the time to terminate
				</li>
			</ul>
			<i>Note: many systems use asynchronous cancellation for process (e.g. system call <code>kill</code>) and threads</i>
	<q>What is thread-safe? Why is it important?
	<a> Data that a thread needs for its own operation are <b>thread-specific</b>. Poor thread-specific support can cause problems <i>(e.g. while threads have their own stacks, they share the heap)</i>. What if two <code>malloc()</code>s are executed at the same time requesting for memory from the heap? Or, two <code>printf</code>s are run simultaneously? A library that can be used by multiple threads properly is <b>thread-safe</b>.
	<q>What is a coroutine?
	<a> a <b>coroutine</b> has multiple entry points and exits so that the next <i>call</i> to a coroutine resumes its execution from the statment/instruction following the previous points. Whereas, a conventional function call always starts from the very beginning. A coroutine can be compared to a scheduler, an exit is a switching out and an enter/re-enter is a switching in.
	<q>What is a fiber?
	<a> A <b>fiber</b> is a lightweight thread just like a thread is a lightweight process. A fiber is created in a thread and shares resources with other fibers of that thread. A fiber has a stack, a subset of regists and data provided when it is created. Fibers are scheduled with <b>co-operative</b> scheduling. Co-operative scheduling means a fiber voluntarily and explicity yields its execution to another fiber with a <code>YIELD</code> or similar function call. Fibers are simpler than threads and resemble coroutines
	<q>What is the relationship between a thread and its fibers?
	<a> A thread-fiber relationship can be thought of as a process-thread relationship <i>(description above)</i>
	<q>Why is the use of fibers "cheaper" than threads?
	<a> They require less resources

<w>
	<q>05-Sync-Basics.pdf
	<q>06-Sync-Soft-Hardware.pdf

	<q>Understand why synchronization is needed? In particular, understand the two examples shown on my slides.
	<a>
		<b>synchronization</b> is needed to prevent race conditions from happening.<br>
		Example One:
		<pre>
			// shared array
			int a[3] = {3, 4, 5};

			// process 1
			a[1] = a[0] + a[1];

			// process 2
			a[2] = a[1] + a[2];

			a = {3, 7, 12} or {3, 7, 9}
		</pre>
		Example Two: 
		<pre>
			// shared counter
			int Count = 10;

			// process 1
			Count++;

			// process 2
			Count--;

			Count = 9, 10, 11
		</pre>
	<q>Define race conditions.
	<a> A <b>race condition</b> occurs if two or more processes/threads manipulate a shared resource concurrently and the outcome of the execution depends on the particular order in which the access takes place.
	<i>Note: You should use instruction level interleaving to demonstarte any reace conditions for higher-level languages and to clearly show any "sharing" or a resource that may occur</i>
	<q>Does your program #1 have race conditions?
	<a> No, because they didn't share any resources, execution order was not relevant.
	<q>What is a critical section?
	<a> A <b>critical section (CS)</b> is a section of code in which a process accesses shared resources
	<q>What is mutual exclusion?
	<a> To avoid race conditions, the execution of critical section must be <b>mutually exclusive</b> <i>(at mose one process in the critical section at any given time)</i> Designing a protocol to conform to mutual exclusion is referred to as the <b>critical-section problem</b>. A critical section protocol consists of an <b>entry section</b> and an <b>exit section</b> that surround the critical section.
	<q>Three conditions must be met in order to design a good solution to the critical section problem. What are these three conditions?
	<a>
			<i>Note: solution cannot depend on the CPU's relative speed, timing, scheduling policy and other external factors</i>
			<ul>
				<li>
					<b>Mutual Exclusion</b>: if a process is executing in its critical section <b>no</b> other processes can be executing in their critical section. The <b>entry protocol</b> should be able to block processes that wish to enter but cannot. When the process that is executing in its critical section exits, the <b>entry protocol</b> must be able to know this fact and allows a waiting process to enter.
				</li>
				<li>
					<b>Progress</b>: if no process is executing in its critical section and some processes want to enter their corresponding critical sections, then:
					<ul>
						<li>
							only those processes that are waiting to enter can participate in the competition (to enter the critical section) and no other processes can influence this decision
						</li>
						<li>
							this decision cannot be postponed indefinitely <i>(e.g. finite decision time)</i>. Thus, one of the waiting processes can enter its critical section
						</li>
					</ul>
				</li>
				<li>
					<b>Bounded Waiting</b>: after a process made a request to enter its critical section and before it is granted the permission to enter, there exists a <b>bound</b> on the number of turns that other processes are allowed to enter. <i>Note: finite is not the same as bounded</i>
				</li>
			</ul>
	<q>What is the difference(s) between progress (liveness) and bounded waiting (starvation free)?
	<a> 
		Progress and Bounded Waiting are independent of each other:
		<ul>
			<li>
				Progress does not imply Bounded Waiting. Progress states a process can enter with a finite decision time. It does not say which process can enter, and there is no guarantee for bounded waiting
			</li>
			<li>
				Bounded Waiting does not imply Progress. Even though we have a bound, all processes may be locked up in the enter section <i>(e.g. failure of Progress)</i>
			</li>
		</ul>

<w>

	<q>05-Sync-Basics.pdf
	<q>06-Sync-Soft-Hardware.pdf


	<q>Consider the following solution to the critical section problem. The flag[] variable may be REQUESTING, IN_CS and OUT_CS, and the turn variable is initialized to 0 or 1.
	<a>
	<pre>
		int  flag[2];  // shared variable flag[]: REQUESTING, IN_CS, OUT_CS
		int  turn;     // shared variable: initial value is either 0 or 1

		Process i (i = 0 or 1)

		// Enter Protocol
		repeat							// repeat the following:
			flag[i] = REQUESTING;				//   make a request to enter                    
			while (turn != i && flag[j] != OUT_CS)		//   wait as long as it is not my turn and 
				; 					//      the other process is not out
			flag[i] = IN_CS;				//   OK, I am IN_CS (maybe); but,
		until flag[j] != IN_CS;					// I have to wait until the other is not in
		turn = i;						// Since the other is not IN, it is my turn

		critical section

		// exit protocol
		turn = j;						// Yield the CS to the other
		flag[i] = OUT_CS;					// I am now out of the CS.
	</pre>

	<q>Show that this solution does satisfy all three conditions (i.e., mutual exclusion, progress, and bounded waiting).</b>
	<a> answer




	<q>We discussed in class that the simple solution using the atomic TS (i.e., test-and-set) instruction satisfies the mutual exclusion condition. Obviously, it does not satisfy the bounded waiting condition. Does this solution satisfy the progress condition?
	Consider the following more sophisticated solution using the atomic TS (i.e., test-and-set) instruction. Consider the following more sophisticated solution. Shared variable waiting[] and lock are initialized to FALSE. Note that this solution works for n processes.
	<a>
	<pre>
		boolean waiting[n];    		// shared variable with initial value FALSE
		boolean lock;          		// shared lock variable with initial value FALSE

		Process i (i = 0, 1, ..., n-1)

		// Enter protocol
		waiting[i] = TRUE;               // I am waiting to enter
		key        = TRUE;               // set my local variable key to FALSE
		while (waiting[i] && key)        // wait and keep trying to
		   key = TS(&lock);              //    lock the shared lock variable
		waiting[i] = FALSE;              // no more waiting and I am in.

		critical section     

		// Exit protocol
		j = (i+1) % n;                   // scan the waiting status of other processes
		while ((j != i) && !waiting[j])  // loop as long as process j is not waiting
		   j = (j+1) % n;                //    move to next process

		if (j == i)                      // no one is waiting
		   lock = FALSE;                 //    set the lock to FALSE
		else                             // process j is waiting to enter
		   waiting[j] = FALSE;           //    release j so that it can enter
	</pre>

	<q>Show that the above solution satisfies the mutual exclusion condition. 
	(Hint: When does the local variable key to be FALSE?)
	<a> answer

	<q>Show that the above solution satisfies the progress condition. 
	(Hint: What are the values of waiting[] and lock that make a process to enter?)
	<a> answer

	<q>Show that the above solution satisfies the bounded waiting. 
	(Hint: Examine the exit protocol and find out the meaning of the while loop. Then, show that a waiting process only waits for no more than n rounds.)
	<a> answer

	<q>If we have such a good solution that satisfies all three conditions, why is the TS instruction implemented this way?
	(Hint: Think about multiprocessors.)
	<a> answer
