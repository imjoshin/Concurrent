<w>
	<q>01-Intro.pdf
	<q>02-Hardware-OS.pdf

<w>
	<q>02-Hardware-OS.pdf
	<q>03-Process.pdf

	<q>What is a CPU mode?
	<a> CPU modes (also called processor modes, CPU states, CPU privilege levels and other names) are operating modes for the central processing unit of some computer architectures that place restrictions on the type and scope of operations that can be performed by certain processes being run by the CPU. This design allows the operating system to run with more privileges than application software. Ideally, only highly trusted kernel code is allowed to execute in the unrestricted mode; everything else (including non-supervisory portions of the operating system) runs in a restricted mode and must use a system call to request the kernel perform on its behalf any operation that could damage or compromise the system, making it impossible for untrusted programs to alter or damage other programs (or the computing system itself).

	<q>What is dual-mode execution?
	<a> Modern CPUs have two execution modes: the <b>user mode</b> and the <b>supervisor</b> (or system, kernel, privileged) mode, controlled by a mode bit.

	<q>What is a privileged instruction? Why are privileged instructions needed?
	<a> A machine code instruction that may only be executed when the processor is running in supervisor mode. <b>Privileged instructions</b> include operations such as I/O and memory management. 

	<q>What is an atomic instruction? Why are atomic instructions needed? What would happen if multiple CPUs/cores execute their atomic instructions?
	<a> These instructions execute as one uninterruptible unit. More precisely, when such an instruction is run, all other instructions being executed in various stages by the CPUs will be stopped (and perhaps re-issued later) until this instruction finishes. If two such instructions are issued at the same time, even though on different CPUs or cores, they will be executed sequentially.


	<q>What is an interrupt, and what is a trap?
	<a> An event that requires the attention of the OS is an <b>interrupt</b>. These events include the completion of an I/O, a keypress, a request for service, a division by zero and so on. Interrupts may be generated by hardware or software. An interrupt generated by software (i.e., division by 0) is usually referred to as a <b>trap</b>.

	<q>Which one of the following event is an interrupt/trap? Why?
	<a>
	<ul>
		<li>
			<b>Real time clock goes off</b> - Interupt
		</li>
		<li>
			<b>A keypress</b> - Interupt
		</li>
		<li>
			<b>A segment fault</b> - Trap
		</li>
		<li>
			<b>A modem dial-up call</b> - Interupt
		</li>
		<li>
			<b>Floating-point exception</b> - Trap
		</li>
		<li>
			<b>Accessing an area not belonging to your program</b> - Trap
		</li>
		<li>
			<b>The completion of an I/O</b> - Interupt
		</li>
		<li>
			<b>In virtual memory accessing a page that is not in the physical memory</b> - Interupt
		</li>
		<li>
			<b>A system call</b> - Trap
		</li>
		<li>
			<b>An memory parity error</b> - Trap
		</li>
	</ul>

	<q>What does it mean by interrupt-driven?<br>
	<a> Modern operating systems are interrupt driven, meaning the OS is in action only if an interrupt occurs.

	<q>Explain the steps the hardware and the operating system will do when an interrupt occurs.
	<a> The OS is activated by an interrupt. The executing program is suspended. Control is transferred to the OS. A program will be resumed when the service completes.

	<q>What is a system call?
	<a> System calls provide an interface to the services made available by an operating system. A system call generates an interrupt (actually a trap), and the caller is suspended.
	<br><br>Types of system calls:'
	<ul>
		<li>
		<b>Process control</b> (e.g., create and destroy processes)
		</li>
		<li>
		<b>File management</b> (e.g., open and close files)
		</li>
		<li>
		<b>Device management</b> (e.g., read and write operations)
		</li>
		<li>
		<b>Information maintenance</b> (e.g., get time or date)
		</li>
		<li>
		<b>Communication</b> (e.g., send and receive messages)
		</li>
	</ul>

	<q>Why is an interval timer needed?
	<a> Because the operating system must maintain the control over the CPU, it has to prevent a user program from getting the CPU forever without calling for system service (i.e., I/O). Before a user program runs, the OS sets the interval timer to certain value. Once the interval timer counts down to 0, an interrupt is generated and the OS can take appropriate action.

	<q>What is a process?
	<a> A process is a program in execution.

	<q>How process space is allocated?
	<a> A process is more than a program, because a process has a program counter, stack, data section, code section, etc (i.e., runtime information). If the stack and heap meet, the process is out of memory.

	<q>What are process states?
	<a> At any moment, a process can be in one of the five states:<br>
	<ul>
		<li>
			<b>New:</b> The process is being created
		</li>
		<li>
			<b>Running:</b> The process is executing on a CPU
		</li>
		<li>
			<b>Waiting:</b> The process is waiting for some event to occur (e.g., waiting for I/O completion)
		</li>
		<li>
			<b>Ready:</b> The process has everything but the CPU. It is waiting to be assigned to a processor.
		</li>
		<li>
			<b>Terminated:</b> The process has finished execution
		</li>
	</ul>

<w>
	<q>03-Process.pdf

	<q>What is a PCB? What information should be stored in a PCB?
	<a> A <a href='pcb.png' target='_blank'><b>Process Control Block</b></a> stores a process's pointer, process state, process ID, program counter, registers, scheduling info, memory limits, and list of open files.

	<q>Can a process assume the system scheduling policy in order to run properly? Why?
	<a> When a CPU is free, the CPU scheduler looks at the ready queue, picks a process, and resumes it. The way of picking a process from the ready queue is referred to as scheduling policy. We cannot make any assumptions about it.

	<q>What is a CPU scheduler?
	<a> When a CPU is free, the CPU scheduler looks at the ready queue, picks a process, and resumes it.

	<q>What is the context of a process?
	<a> The context of a process includes process ID, process state, the values of CPU registers, the program counter, and other memory/file management information (i.e., execution environment).

	<q>What is a context switch? Show all needed steps in a context switch.
	<a> After the CPU scheduler selects a process and before allocates CPU to it, the CPU scheduler must save the context of the currently running process, put it back to the ready queue, load the context of the selected process, and go to the saved program counter.

	<q>Understand the following system calls 
	<a href='http://linux.die.net/man/2/fork' target='_blank'>fork()</a>, 
	<a href='http://linux.die.net/man/2/wait' target='_blank'>wait()</a>, and 
	<a href='http://linux.die.net/man/3/execvp' target='_blank'>execvp()</a>.
	<a>

	<q>Understand the following shared memory related functions and system calls 
	<a href='http://linux.die.net/man/3/ftok' target='_blank'>ftok()</a>, 
	<a href='http://linux.die.net/man/2/shmget' target='_blank'>shmget()</a>, 
	<a href='http://linux.die.net/man/2/shmat' target='_blank'>shmat()</a>, 
	<a href='http://linux.die.net/man/2/shmdt' target='_blank'>shmdt()</a>, and 
	<a href='http://linux.die.net/man/2/shmctl' target='_blank'>shmctl()</a>.
	<a> <a href='http://www.cs.cf.ac.uk/Dave/C/node27.html' target='_blank'>Shared Memory example</a>


<w>
	<q>04-Thread.pdf

	<q>What is a thread or lightweight process?
	<a> A basic unit of CPU execution, and is created by a process.
	<q>What are the resources needed to run a thread?
	<a> A thread has a thread ID, a program counter, a register set and a stack. It is similar to a process,
		however, a thread shares with other threads in the same process, its code section, data section
		and other OS resources (files and signals) 
	<q>Why are threads cheaper than processes?
	<a> threads share the same resources of the main process
	<q>What are the major benefits of using threads?
	<a> 
			<ul>
				<li>
					<b>Responsiveness</b>: other parts of a program <i>(ex: other threads)</i> may still run even if one part <i>(ex: another threads)</i> is blocked
				</li>
				<li>
					<b>Resource Sharing</b>: threads of a process, by default, share many system resources <i>(ex: files and memory)</i>
				</li>
				<li>
					<b>Economy</b>: creating and terminating processes, allocating memroy and resources and context switching processes are very time consuming
				</li>
				<li>
					<b>Utilization of Multiprocessor Architecture</b>: multiple CPUs may run multiple threads of the same process. No program change is necessary
				</li>
			</ul>
	<q>What is a user-level thread?
	<a> User threads are supported at the <b>user level</b> and the kernel is not aware of user threads. A library provides all support for thread creating, termination, joining, and scheduling.
	<q>What is a kernel supported thread?
	<a> Kernel threads are supported by the kernel. The kernel does thread creation, termination, joing and scheduling in the kernel space. 
	<q>Describe the major advantages and disadvantages of using user-level threads and kernel supported threads.
	<a>
			<ul>
				<li>
					<b>Kernel threads</b> are usually <b>slower</b> than user threads due to system overhead. However, blocking one thread will not cause other threads of the same process to block, the kernel simply runs other kernel threads. In a multiprocessor environment, the kernel may schedule threads on different processors.
				</li>
				<li>
					Since there is no kernel intervention, <b>user threads</b> are usually more efficient. However, if one thread is blocked, all threads of the same process are also blocked, because the containing process is blocked. This is because the kernel only recognizes the containing process (of the threads)
				</li>
			</ul>
	<q>What are the thread models?
	<a>
			different systems support threads in different ways. 3 common thread models:
			<ul>
				<li>
					<b>many-to-one</b>
				</li>
				<li>
					<b>one-to-one</b>
				</li>
				<li>
					<b>many-to-many</b>
				</li>
			</ul>
	<q>How threads of a process are scheduled in each model?
	<a> 
			<ul>
				<li>
					<b>many-to-one</b>: one kernel threads (or process) has multiple user threads, user thread model
				</li>
				<li>
					<b>one-to-one</b>: one user thread maps to one kernel thread <i>(e.g. old Unix/Linux and Windows systems)</i>
				</li>
				<li>
					<b>many-to-many</b>: multiple user threads map to a number of kernel threads
				</li>
			</ul>
	<q>Understand the six points discussed in the multicore programming section.
	<a> With a single-core CPU threads are scheduled by a scheduler and can only run one at a time. With a multicore CPU multiple threads may run at the same time, one on each core. There are 5 main issues:
			<ul>
				<li>
					<b>Dividing Activies</b>: since each thread can run on a core, one must study the problem in hand so that program activites can be divided and run concurrently <i>(e.g. matrix multiplication)</i>
				</li>
				<li>
					<b>Balance</b>: make sure that each threads has <b>equal</b> contribution (if possible) to the whole computation
				</li>
				<li>
					<b>Data Splitting</b>: data may also be split into different section so that each can be separately processed <i>(e.g. matrix multiplication and quicksort after partitioning)</i>
				</li>
				<li>
					<b>Data Dependency</b>: watch for data items that are used by different threads. <i>(e.g. two threads may update a common variable at the same time)</i>. This can cause unexpected results and thus requires <b>synchronization</b> so only one thread can update a shared variable
				</li>
				<li>
					<b>Testing and Debugging</b>: threaded programming is <b>dynamic</b> causing unpredictable bugs. Some debugging issues do not have efficient solutions <i>(e.g. race conditions and system deadlock)</i>
				</li>
			</ul>
	<q>What are the two commonly used thread cancellation methods?
	<a> thread cancellation means terminating a thread before its completion. The thread to be cancelled is the <b>target thread</b> the two types of cancellation:
			<ul>
				<li>
					<b>Asynchronous</b>: the target threads terminates immediately. If the target thread owns some system-wide resource the system may not be able to reclaim these resources because other threads may be using them
				</li>
				<li>
					<b>Deferred</b>: the target thread periodically checks if it should terminate, allowing termination to be done in a controlled manner. The point the thread terminates itself is refferred to as the <b>cancellation point</b>. Reclaiming system-wide resources is not a problem since the threads determines the time to terminate
				</li>
			</ul>
			<i>Note: many systems use asynchronous cancellation for process (e.g. system call <code>kill</code>) and threads</i>
	<q>What is thread-safe? Why is it important?
	<a> Data that a thread needs for its own operation are <b>thread-specific</b>. Poor thread-specific support can cause problems <i>(e.g. while threads have their own stacks, they share the heap)</i>. What if two <code>malloc()</code>s are executed at the same time requesting for memory from the heap? Or, two <code>printf</code>s are run simultaneously? A library that can be used by multiple threads properly is <b>thread-safe</b>.
	<q>What is a coroutine?
	<a> a <b>coroutine</b> has multiple entry points and exits so that the next <i>call</i> to a coroutine resumes its execution from the statment/instruction following the previous points. Whereas, a conventional function call always starts from the very beginning. A coroutine can be compared to a scheduler, an exit is a switching out and an enter/re-enter is a switching in.
	<q>What is a fiber?
	<a> A <b>fiber</b> is a lightweight thread just like a thread is a lightweight process. A fiber is created in a thread and shares resources with other fibers of that thread. A fiber has a stack, a subset of regists and data provided when it is created. Fibers are scheduled with <b>co-operative</b> scheduling. Co-operative scheduling means a fiber voluntarily and explicity yields its execution to another fiber with a <code>YIELD</code> or similar function call. Fibers are simpler than threads and resemble coroutines
	<q>What is the relationship between a thread and its fibers?
	<a> A thread-fiber relationship can be thought of as a process-thread relationship <i>(description above)</i>
	<q>Why is the use of fibers "cheaper" than threads?
	<a> They require less resources

<w>
	<q>05-Sync-Basics.pdf
	<q>06-Sync-Soft-Hardware.pdf

	<q>Understand why synchronization is needed? In particular, understand the two examples shown on my slides.
	<a>
		<b>synchronization</b> is needed to prevent race conditions from happening.<br>
		Example One:
		<pre>
			// shared array
			int a[3] = {3, 4, 5};

			// process 1
			a[1] = a[0] + a[1];

			// process 2
			a[2] = a[1] + a[2];

			a = {3, 7, 12} or {3, 7, 9}
		</pre>
		Example Two: 
		<pre>
			// shared counter
			int Count = 10;

			// process 1
			Count++;

			// process 2
			Count--;

			Count = 9, 10, 11
		</pre>
	<q>Define race conditions.
	<a> A <b>race condition</b> occurs if two or more processes/threads manipulate a shared resource concurrently and the outcome of the execution depends on the particular order in which the access takes place.
	<i>Note: You should use instruction level interleaving to demonstarte any reace conditions for higher-level languages and to clearly show any "sharing" or a resource that may occur</i>
	<q>Does your program #1 have race conditions?
	<a> No, because they didn't share any resources, execution order was not relevant.
	<q>What is a critical section?
	<a> A <b>critical section (CS)</b> is a section of code in which a process accesses shared resources
	<q>What is mutual exclusion?
	<a> To avoid race conditions, the execution of critical section must be <b>mutually exclusive</b> <i>(at mose one process in the critical section at any given time)</i> Designing a protocol to conform to mutual exclusion is referred to as the <b>critical-section problem</b>. A critical section protocol consists of an <b>entry section</b> and an <b>exit section</b> that surround the critical section.
	<q>Three conditions must be met in order to design a good solution to the critical section problem. What are these three conditions?
	<a>
			<i>Note: solution cannot depend on the CPU's relative speed, timing, scheduling policy and other external factors</i>
			<ul>
				<li>
					<b>Mutual Exclusion</b>: if a process is executing in its critical section <b>no</b> other processes can be executing in their critical section. The <b>entry protocol</b> should be able to block processes that wish to enter but cannot. When the process that is executing in its critical section exits, the <b>entry protocol</b> must be able to know this fact and allows a waiting process to enter.
				</li>
				<li>
					<b>Progress</b>: if no process is executing in its critical section and some processes want to enter their corresponding critical sections, then:
					<ul>
						<li>
							only those processes that are waiting to enter can participate in the competition (to enter the critical section) and no other processes can influence this decision
						</li>
						<li>
							this decision cannot be postponed indefinitely <i>(e.g. finite decision time)</i>. Thus, one of the waiting processes can enter its critical section
						</li>
					</ul>
				</li>
				<li>
					<b>Bounded Waiting</b>: after a process made a request to enter its critical section and before it is granted the permission to enter, there exists a <b>bound</b> on the number of turns that other processes are allowed to enter. <i>Note: finite is not the same as bounded</i>
				</li>
			</ul>
	<q>What is the difference(s) between progress (liveness) and bounded waiting (starvation free)?
	<a> 
		Progress and Bounded Waiting are independent of each other:
		<ul>
			<li>
				Progress does not imply Bounded Waiting. Progress states a process can enter with a finite decision time. It does not say which process can enter, and there is no guarantee for bounded waiting
			</li>
			<li>
				Bounded Waiting does not imply Progress. Even though we have a bound, all processes may be locked up in the enter section <i>(e.g. failure of Progress)</i>
			</li>
		</ul>

<w>

	<q>05-Sync-Basics.pdf
	<q>06-Sync-Soft-Hardware.pdf


	<q>Consider the following solution to the critical section problem. The flag[] variable may be REQUESTING, IN_CS and OUT_CS, and the turn variable is initialized to 0 or 1.
	<a>
	<pre>
		int  flag[2];  // shared variable flag[]: REQUESTING, IN_CS, OUT_CS
		int  turn;     // shared variable: initial value is either 0 or 1

		Process i (i = 0 or 1)

		// Enter Protocol
		repeat							// repeat the following:
			flag[i] = REQUESTING;				//   make a request to enter                    
			while (turn != i && flag[j] != OUT_CS)		//   wait as long as it is not my turn and 
				; 					//      the other process is not out
			flag[i] = IN_CS;				//   OK, I am IN_CS (maybe); but,
		until flag[j] != IN_CS;					// I have to wait until the other is not in
		turn = i;						// Since the other is not IN, it is my turn

		critical section

		// exit protocol
		turn = j;						// Yield the CS to the other
		flag[i] = OUT_CS;					// I am now out of the CS.
	</pre>

	<q>Show that this solution does satisfy all three conditions (i.e., mutual exclusion, progress, and bounded waiting).</b>
	<a> answer




	<q>We discussed in class that the simple solution using the atomic TS (i.e., test-and-set) instruction satisfies the mutual exclusion condition. Obviously, it does not satisfy the bounded waiting condition. Does this solution satisfy the progress condition?
	Consider the following more sophisticated solution using the atomic TS (i.e., test-and-set) instruction. Consider the following more sophisticated solution. Shared variable waiting[] and lock are initialized to FALSE. Note that this solution works for n processes.
	<a>
	<pre>
		boolean waiting[n];    		// shared variable with initial value FALSE
		boolean lock;          		// shared lock variable with initial value FALSE

		Process i (i = 0, 1, ..., n-1)

		// Enter protocol
		waiting[i] = TRUE;               // I am waiting to enter
		key        = TRUE;               // set my local variable key to FALSE
		while (waiting[i] && key)        // wait and keep trying to
		   key = TS(&lock);              //    lock the shared lock variable
		waiting[i] = FALSE;              // no more waiting and I am in.

		critical section     

		// Exit protocol
		j = (i+1) % n;                   // scan the waiting status of other processes
		while ((j != i) && !waiting[j])  // loop as long as process j is not waiting
		   j = (j+1) % n;                //    move to next process

		if (j == i)                      // no one is waiting
		   lock = FALSE;                 //    set the lock to FALSE
		else                             // process j is waiting to enter
		   waiting[j] = FALSE;           //    release j so that it can enter
	</pre>

	<q>Show that the above solution satisfies the mutual exclusion condition. 
	(Hint: When does the local variable key to be FALSE?)
	<a> answer

	<q>Show that the above solution satisfies the progress condition. 
	(Hint: What are the values of waiting[] and lock that make a process to enter?)
	<a> answer

	<q>Show that the above solution satisfies the bounded waiting. 
	(Hint: Examine the exit protocol and find out the meaning of the while loop. Then, show that a waiting process only waits for no more than n rounds.)
	<a> answer

	<q>If we have such a good solution that satisfies all three conditions, why is the TS instruction implemented this way?
	(Hint: Think about multiprocessors.)
	<a> answer

<w>
	<q>06-Sync-Soft-Hardware.pdf
	<q>07-Some-Cpp-TM.pdf

<w>
	<q>07-Some-Cpp-TM.pdf
	<q>08-Semaphores.pdf

	<q>In the definition of a semaphore, the counter and waiting list are private members, and, as a result, it is impossible 
		to know the current value of the counter. Suppose a public member function GetCounter() is added, and suppose this 
		member function, just like the Signal() and Wait() functions, is also atomic. If the counter value is negative, its 
		absolute value is the number of processes in the waiting list. Discuss the accuracy of the returned value by member 
		function GetCounter(). In other words, is it always the case that the returned value provides an accurate count of 
		waiting processes?
	<a> Due to the fact that GetCounter() is atomic, along with Signal() and Wait(), only one of these functions may be run at a single time. Therefore, GetCounter() will always return an accurate value of the current counter.

	<q>Show, by an execution sequence, that if the method Wait() is not atomic (but the method Signal() is) then mutual exclusion cannot be guaranteed by the use of a semaphore.
	<a>
	<table style="width:500px">
		<tr>
			<th style="width:150px">Thread 1</th>
			<th style="width:150px">Thread 2</th>
			<th style="width:200px">Comment</th>
		</tr>
		<tr>
			<td>stuff</td>
			<td>stuff</td>
			<td>count is 0</td>
		</tr>
		<tr>
			<td>Wait() is called</td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td>Load count</td>
			<td></td>
			<td>count is loaded as 0</td>
		</tr>
		<tr>
			<td></td>
			<td>Wait() is called</td>
			<td></td>
		</tr>
		<tr>
			<td></td>
			<td>Load count</td>
			<td>count is loaded as 0</td>
		</tr>
		<tr>
			<td></td>
			<td>Decrement count</td>
			<td>count is now -1 in Thread 2</td>
		</tr>
		<tr>
			<td>Decrement count</td>
			<td></td>
			<td>count is now -1 in Thread 1</td>
		</tr>
	</table>

	Two threads executed Wait() when count was 0, count is now set to -1 when it should be -2.

	<q>Show, by an execution sequence, that if the method Signal() is not atomic (but the method Wait() is) then mutual exclusion cannot be guaranteed by the use of a semaphore.
	<a> 
	<table style="width:500px">
		<tr>
			<th style="width:150px">Thread 1</th>
			<th style="width:150px">Thread 2</th>
			<th style="width:200px">Comment</th>
		</tr>
		<tr>
			<td>stuff</td>
			<td>stuff</td>
			<td>count is 0</td>
		</tr>
		<tr>
			<td>Signal() is called</td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td>Load count</td>
			<td></td>
			<td>count is loaded as 0</td>
		</tr>
		<tr>
			<td></td>
			<td>Signal() is called</td>
			<td></td>
		</tr>
		<tr>
			<td></td>
			<td>Load count</td>
			<td>count is loaded as 0</td>
		</tr>
		<tr>
			<td></td>
			<td>Increment count</td>
			<td>count is now 1 in Thread 2</td>
		</tr>
		<tr>
			<td>Increment count</td>
			<td></td>
			<td>count is now 1 in Thread 1</td>
		</tr>
	</table>

	Two threads executed Signal() when count was 0, count is now set to 1 when it should be 2.

	<q>Suppose we have three processes <i>P</i><sub>1</sub>, <i>P</i><sub>2</sub> and <i>P</i><sub>3</sub> and process <i>P</i><sub>i</sub> only prints the value of <i>i</i> as follows:
	</b>
	<pre>
	Process 1
	while(1) {
		...
		cout << "1";
		...
	}

	Process 2
	while(1) {
		...
		cout << "2";
		...
	}

	Process 3
	while(1) {
		...
		cout << "3";
		...
	}
	</pre>
	<b>
	Add semaphores and cout if necessary to the above processes so that the three processes will print out the following sequence 1 2 3 2 1 2 3 2 1 2 3 2 1 ...
	<a>

	<pre>
	Semaphore s = 1;

	Process 1
	while(1) {
		...
		s.wait();
		cout << "1 2 3 2 ";
		s.signal();
		...
	}

	Process 2
	while(1) {
		...
		s.wait();
		cout << "1 2 3 2 ";
		s.signal();
		...
	}

	Process 3
	while(1) {
		...
		s.wait();
		cout << "1 2 3 2 ";
		s.signal();
		...
	}
	</pre>



	<q>Modify the above template so that it will print out the following sequence 1 2 3 1 2 3 1 2 3 1 2 3 ...
	<a>

	<pre>
	Semaphore s = 1;

	Process 1
	while(1) {
		...
		s.wait();
		cout << "1 2 3 ";
		s.signal();
		...
	}

	Process 2
	while(1) {
		...
		s.wait();
		cout << "1 2 3 ";
		s.signal();
		...
	}

	Process 3
	while(1) {
		...
		s.wait();
		cout << "1 2 3 ";
		s.signal();
		...
	}
	</pre>

	<q>We know that a semaphore with an appropriate initial value can guarantee mutual exclusion. Does the semaphore solution satisfy the progress condition and the bounded waiting condition? Use execution sequences to answer this question.
	<a> 
	<table style="width:500px">
		<tr>
			<th style="width:150px">Thread 1</th>
			<th style="width:150px">Thread 2</th>
			<th style="width:200px">Comment</th>
		</tr>
		<tr>
			<td>stuff</td>
			<td>stuff</td>
			<td>count is 1</td>
		</tr>
		<tr>
			<td></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td></td>
			<td></td>
			<td></td>
		</tr>
	</table>

	<q>Use the Test-and-Set instruction to implement semaphore wait and semaphore signal.
	<a> 
	<pre>
	wait(){
		S.count--;
		if(S.count < 0) {
			while(TS(&bool)); //if bool == false, set bool to true and return false
		}
	}

	signal(){
		S.count++;
		if(S.count <= 0){
			bool = false;
		}
	}
	</pre>

	<q>It was shown in class that deadlocks can occur in the naive solution to the dining philosophers problem. We also showed that to avoid deadlocks from happening a righty philosopher may be introduced or add a restriction that at most four philosophers may sit down and eat. Do the following problems:
	</b>
	<a> 
	<ul>
		<li>
			<b>Rigorously prove that the righty solution does not cause any deadlock.</b><br>
			In the righty solution, it is impossible for all 5 philosophers to have a single chopstick. When the 5 philosophers first sit down, 4 of the 5 will reach for his/her left chopstick. In this proof, Philosopher #5 will reach for his/her right chopstick. Two possibilities are presented: Philosopher #4 will beat #5 to their shared chopstick, or #5 will pick up his right chopstick and beat #1 to their shared chopstick. In the first case, #4 will have both of his/her chopsticks, in which #3 and #5 would be waiting. In the second case, #5 will have both of his/her chopstick, in which #1 and #4 would be waiting. Once the philosopher with both chopsticks is finished eating, the others would pick up those two chopsticks and the cycle would continue as such.
		</li>
		<li>
			<b>Rigorously prove that the four-chair solution does not cause any deadlocks.</b><br>
			If only 4 of the 5 chairs are taken, deadlock cannot occur. This is because there is one open chair, and the right-most philosopher is able to pick up both chopsticks.
		</li>
		<li>
			<b>Show, with execution sequences, that the naive, righty and four-chair solutions can have starvation. More precisely, use execution sequences to show that there are philosophers who are hungry and have no chance to eat.</b>
		</li>
	</ul><b>



	<q>It was suggested that to avoid possible deadlocks in the philosophers problem one may add more resources which should be arranged in a different way. The following shows some attempts:
	</b><ul>
			<li>
				Add a single chopstick at the center of the table. A philosopher picks up his left chopstick and then competes to grab the chopstick at the center. If he can get both, then he eats.
				<ul>
					<li>Does this approach avoid any possible deadlock?</li>
					<li>Any bad consequence that attempt could have such as maximum parallelism?</li>
					<li>With semaphores, is it possible to implement the following protocol? Why?</li>
					<ul>
						<li>Each philosopher picks up his left chopstick.</li>
						<li>If he fails to do so, then try to get the center one. Otherwise, he waits until his left chopstick becomes available.</li>
						<li>Do the same for his right chopstick.</li>
					</ul>
					<li>Discuss the efficiency and deadlock issues of this "suggested" solution.</li>
				</ul>
			</li>
			<li>
				Instead of having the chopsticks arranged as discussed in class, we make all five chopsticks in a tray next to the table so that any philosopher can pick any chopstick. In this attempt, each philosopher picks up his first chopstick, and, then picks the second. Is deadlock possible? How about parallelism?
			</li>
		</ul>
	<a> answer


	<q>We discussed in class that the waiting processes on a semaphore should be better considered as a set (with any ordering) rather than a queue. As a result, there is no particular order when releasing a waiting process, and starvation could occur. On the other hand, with multiple semaphores we are able to come up with a starvation-free solution to the critical section problem.<br>
	 The following solution, due to Morris (1979), uses three semaphores. Semaphores FirstBarrier and SecondBarrier are used to setup two barriers, and semaphore Lock is used to protect counter FirstCount for mutual exclusion purpose. Two counters are used: FirstCount counts the number of processes ready to cross the first barrier represented by semaphore FirstBarrier, and SecondCount counts the number of processes having already crossed the first barrier but not barrier SecondBarrier.
	</b><pre>
	Semaphore:  	FirstBarrier    = 1,
					SecondBarrier   = 0,
					Lock            = 1;

			int         FirstCount      = 0,
					SecondCount     = 0;
	</pre><b>
	The following shows the enter and exit protocol:
	</b><pre>
	// Enter Protocol

	Wait(Lock);                                 // I am ready and prepared to cross the 1st barrier
			FirstCount++;                       // increase the counter
	Signal(Lock);

	Wait(FirstBarrier);                         // wait on the 1st barrier
			SecondCount++;                      // I have successfully crossed the 1st barrier
			Wait(Lock);                         // Have to lock the counter to decrease
			FirstCount--;                       // Now I am about to cross the 2nd barrier
			if (FirstCount == 0) {              // if I was the only one crossing the 1st barrier
					Signal(Lock);               //     then release the lock
					Signal(SecondBarrier);      //     and signal the 2nd barrier so that I could cross
			}
			else {                              // if I am not the only one, I have to wait; but
					Signal(Lock);               //     I have to release the lock, and
					Signal(FirstBarrier);       //     also open the 1st barrier
			}
	Wait(SecondBarrier);                        // wait on the second barrier
	SecondCount--;                              // I am no more waiting to cross the 2nd barrier

// in critical section 

// Exit protocol

	if (SecondCount == 0)                       // if there is no one wait on the 2nd barrier
			Signal(FirstBarrier);               //     open the 1st barrier
	else					    // otherwise,
			Signal(SecondBarrier);              //     open the 2nd barrier

	</pre>
	<b>Try to understand the above protocol, and convince yourself that mutual exclusion and bounded-waiting are satisfied. How about the progress condition?

	<a> answer




<w>

	<q>08-Semaphores.pdf
	<q>09-Race-Conditions.pdf

	<q>Suppose the smokers problem is changed to the following version.
	</b>
	<ul>
		<li>There are three semaphores, one for tobacco, one for matches, and one for paper</li>
		<li>The smoker who has tobacco waits on semaphores matches and paper, the smoker who has matches waits on semaphores tobacco and paper. The smoker who has paper waits on semaphores tobacco and matches.</li>
		<li>The agent randomly picks up two ingredients and signals the corresponding semaphores.</li>
		<li>There is a table, of course.</li>
	</ul><b>
	Does this version work properly without any problem?
	<a> No, this solution may work a portion of the time but <b>deadlock</b> is very likely. For example:
	<table>
		<tr>
			<th>
				Agent
			</th>
			<th>
				Smoker (has tobacco)
			</th>
			<th>
				Smoker (has matches)
			</th>
			<th>
				Smoker (has paper)
			</th>
		</tr>
		<tr>
			<td>
				Picks paper & matches
			</td>
			<td>
				
			</td>
			<td>
				
			</td>
			<td>
				
			</td>
		</tr>
		<tr>
			<td>
				paper.signal() & matches.signal()
			</td>
			<td>
				
			</td>
			<td>
				
			</td>
			<td>
				
			</td>
		</tr>
		<tr>
			<td>

			</td>
			<td>
				paper.wait()
			</td>
			<td>
				<i>needs tobacco</i>
			</td>
			<td>
				matches.signal()
			</td>
		</tr>
		<tr>
			<td>
			</td>
			<td>
				<i>needs matches</i>
			</td>
			<td>
				<i>needs tobacco</i>
			</td>
			<td>
				<i>needs paper</i>
			</td>
		</tr>
	</table>

	<q>The above more general smoker version is basically due to simultaneously waiting on multiple semaphores. More specifically, simultaneously waiting on multiple semaphores is not atomic. Why?
	<a> A single semaphore is atomic but multiple semaphores can be changed simultaneously <i>(wait, signal)</i>
	resulting in a possible race condition when working with multiple semaphores

	<q>As you may have noticed that the WrtMutex semaphore is shared by the readers and writers. It blocks no more than one reader and multiple writers (Prove this!). As a result, when an exiting reader or writer signals this semaphore, a reader or a writer could be released. Some argued that this does not satisfy the "reader priority" requirement because there may have already had a reader waiting. Although it is an argument based on when the "reading" starts (i.e., entering at the beginning vs. actual reading), one can certainly improve the situation a little. Here is another reader-priority solution with one added semaphore WriterOnly with initial value 1. The following is the code for writers as readers are not affected:

	</b><pre>Semaphore  WriterOnly = 1; // other semaphores are the same</pre><b>
	A writer process has the following form:
	</b><pre>

	while {
		 Wait(WriterOnly);     // wait on our own semaphore
			  Wait(WrtMutex);  // then, compete with the only possible reader
							   // writing
			  Signal(WrtMutex);
		 Signal(WriterOnly);
	}

	</pre><b>

	Do you think this is a "better" version? In other word, does the only waiting reader on semaphore WrtMutex have more chance to take the priority (i.e., reader-priority)?

	<a> No, this only allows one writer through at a time, forcing the inner statements to be executed atomically but it
	produces the exact same results since this is already what occurrs.

	<q>Here is another solution.
	</b><pre>

	int  ActiveReaders = ActiveWriters = 0;
	int  WaitingReaders = WaitingWriters = 0;

	Semaphore Mutex = 1;       // protecting the above declared counters
	Semaphore OKtoRead = 0;
	Semaphore OKtoWrite = 0;

	</pre><b>

	A reader process has the following form:
	</b><pre>

	while (1) {
		 Wait(Mutex);                           // the enter part
		 if (ActiveWriters + ActiveReaders == 0) {  // no one is reading and writing
			  Signal(OKtoWrite);
			  ActiveReaders++;                  // I am in
		 }
		 else 
			  WaitingReaders++;                 // otherwise, I must wait
		 Signal(Mutex);                         // release the counter lock

		 Wait(OKtoRead);                        // wait to get the read permission
												// read data

		 Wait(Mutex);                           // the exit part
		 ActiveReaders--;                       // I am done reading
		 if (ActiveReaders == 0 && WaitingWriters > 0) {
			  Signal(OKtoWrite);                // allow writing of no reader and some writers
			  ActiveWriters++;                  // we have an active writer
			  WaitingWriters--;                 //    and one less waiting writers
		 }
		 Signal(Mutex);                         // release the counter
	}

	</pre><b>

	A writer process has the following form:
	</b><pre>

	while (1) {
		 Wait(Mutex);                           // the enter part
		 if (ActiveWriters + ActiveReaders + WaitingWriters == 0) {  
												// no one is reading, writing and waiting
			  Signal(OKtoWrite);
			  ActiveWriters++;                  // I (writer) am in
		 }
		 else 
			  WaitingWriters++;                 // otherwise, I must wait
		 Signal(Mutex);                         // release the counter lock

		 Wait(OKtoWrite);                       // wait to get the read permission
												// write data

		 Wait(Mutex);                           // the exit part
		 ActiveWriters--;                       // I am done writing
		 if (WaitingWriters > 0) {              // if there are writers waiting
			  Signal(OKtoWrite);                // allow writing of no reader and some writers
			  ActiveWriters++;                  // we have an active writer
			  WaitingWriters--;                 //    and one less waiting writers
		 }
		 else {
			  while (WaitingReaders > 0) {      // if there are waiting readers
				 Signal(OKtoRead);              //    let them read one-by-one
				 ActiveReaders++;
				 WaitingReaders--;
		 Signal(Mutex);                         // release the counter
	}

	</pre><b>

	Study this solution and make sure you understand what it is trying to do. Is this a reader-priority or a writer-priority solution?

	<a> answer


	<q>We discussed the reader-priority version of the readers/writers problem. Here is a solution to the writer-priority version of the readers/writers problem in which writers have higher priority. More precisely, as long as there is at least one writer has declared a desire to write, no new readers are allowed to read. Now, a reader process uses three semaphores Block, ReadMutex and ReadCountMutex, and a writer process uses two semaphores WriteMutex and WriteCountMutex:

	</b><pre>

	Semaphore:  Block           = 1,
			ReadMutex       = 1,
			ReadCountMutex  = 1,
			WriteMutex      = 1,
			WriteCountMutex = 1;

	</pre><b>

	A reader process has the following form:

	</b><pre>

	while {
		 Wait(Block);
			  Wait(ReadMutex);
				   Wait(ReadCountMutex);
						ReadCount++;
						if (ReadCount == 1) 
							 Wait(WriteMutex);
				   Signal(ReadCountMutex);
			  Signal(ReadMutex);
		 Signal(Block);

		 ..... read the database .....

		 Wait(ReadCountMutex);
			  ReadCount--;
			  if (ReadCount == 0)
				   Signal(WriteMutex);
		 Signal(ReadCountMutex);
	}

	</pre><b>

	A writer process has the following form:

	</b><pre>

	while {
		 Wait(WriteCountMutex);
			  WriteCount++;
			  if (WriteCount == 1)
				   Wait(ReadMutex);
		 Signal(WriteCountMutex);
		 Wait(WriteMutex);

		 ..... write the database .....

		 Signal(WriteMutex);
		 Wait(WriteCountMutex);
			  WriteCount--;
			  if (WriteCount == 0)
				   Signal(ReadMutex);
		 Signal(WriteCountMutex);
	}

	</pre><b>

	The initial values of ReadCount and WriteCount are both 0. Please study this solution and answer the following questions:

	<a>

	<q> 1. What is the purpose of using Wait(Block) and Wait(ReadMutex)? Before proceed to the next question, think again if your point is correct.
	<a> answer

	<q> 2. You would say "semaphores Block and ReadMutex are used to lock the database." But, if this were true, the database would be accessed by <i>no more than one</i> reader (or one writer). Is this a correct observation? Or, did I fool you with some bad reasoning? Keep in mind that the readers-and-writers problem requires simultaneous read and exclusive write.
	<a> answer

	<q> 3. Now, tell me why readers can access the database simultaneously.
	<a> answer

	<q> 4. But, the initial value of Block is 1. This would only allow one reader to access the database. If this is the case, why don't we change:
	</b><pre>

	while {
		 Wait(Block);
			  Wait(ReadMutex);
				   Wait(ReadCountMutex);
						ReadCount++;
						if (ReadCount == 1) 
							 Wait(WriteMutex);
				   Signal(ReadCountMutex);
			  Signal(ReadMutex);
		 Signal(Block);
		 ...............
	}

	</pre><b>

	to the following?

	</b><pre>

	while {
		 Wait(ReadMutex);
			  Wait(ReadCountMutex);
				   ReadCount++;
				   if (ReadCount == 1) 
						Wait(WriteMutex);
			  Signal(ReadCountMutex);
		 Signal(ReadMutex);

		 ..... read the database .....

		 Wait(ReadCountMutex);
			  ReadCount--;
			  if (ReadCount == 0)
				   Signal(WriteMutex);
		 Signal(ReadCountMutex);
	}

	</pre><b>

	<a> answer

	<q>Removing Block is the most natural suggestion, since it is used in the reader process and is used exactly once. Explain why you cannot do this. If Block is removed, what will happen to the solution.
	<a> answer



<w>
	<q>08-Semaphores.pdf
	<q>09-Race-Conditions.pdf

	<q>A restaurant has n tables, each of which can only sit one customer, and has the following rules.
	<ul>
		<li>The restaurant is empty initially (i.e., no waiting and eating customers).</li>
		<li>When a customer arrives, if there is a free table, he could sit down and order food.</li>
		<li>When a customer arrives, if all tables are occupied, he must wait until all n customers finish eating and leave.</li>
		<li>After finishing his food, a customer leaves.</li>
	</ul>
	Design a customer thread with semaphores to simulate this activity.

	<a><pre>

		int Waiting = Eating = 0; 			// counting waiting and eating customers
		int i, k; 					// working variables
		Bool Must_Wait = FALSE; 			// variable indicating if a newcomer must wait
		Semaphore Mutex = 1; 				// lock for protecting Waiting, Eating and Must_Wait
		Semaphore Table = 0; 				// block customers


		// A customer thread
		Mutex.Wait();					// newcomer must lock the variables
		if (Must_Wait) { 				// there are waiting customers or no table is available?
			Waiting++; 				// must wait and increase Waiting
			Mutex.Signal(); 			// release the lock before waiting
			Table.Wait(); 				// and join the waiting line
		}
		else { 						// no waiting customers and a table available
		Eating++; 					// this customer can have a table
		if (Eating == n) 				// if he is the last one who gets a table
			Must_Wait = TRUE; 			// he must set Must_Wait to TRUE
			Mutex.Signal(); 			// release the lock and go eat
		}


		// This customer has a table and easts
		Mutex.Wait(); 					// finish eating and lock variables
		Eating--; 					// decrease Eating

		if (Eating == 0) { 				// if this is the last customer
			k = (Waiting <= n) ? Waiting : n;	// allow no more than n to go
			Waiting -= k; 				// release k waiting customers
			Eating += k; 				// they can have tables
			Must_Wait = (k == n) 			// if this is a full count, newcomers must wait
			for (i = 1; i <= k; i++) 		// release those k waiting customers
				Table.Signal();
		}

		Mutex.Signal(); 				// unlock variables
	</pre>


	<q>As you know MTU does not have many female students. To restrict the number of male party goers, a fraternity party made an odd rule to maintain the number of male and female students in its party. This rule states that a female student must be accompanied by two male students to enter the party room. Write a thread to simulate male students and another thread to simulate female students using semaphores so that this 2-1 relation is maintained. Can you extend your solution to 3-1 and even n-1?

	<a>Answer


	<q>The above rule has a loop hole, because a student could sneak out of the party room and get more students in. As a result, the 2-1 ratio cannot be maintained. To close this gap, let us make the rule a bit more strict. A 2-1 triple must form to enter and exit the party. In other word, once two male students and one female student form a triple to enter the party room, they must also form a 2-1 triple in order to leave the party before reentering the party again. Of course, the exit 2-1 triple may not be the same as the enter 2-1 triple. For example, John, Jason and Mary form an enter 2-1 triple; but, John, Mike and Jane could form an exit triple. In this way, the 2-1 rule is maintained. Will your solution be different from the previous one? (Hint: the bounded-buffer problem may help, again.)

	<a>Answer




<w>

	<q>10-Monitors.pdf

	<q>Find some web sites or tutorials to learn Unix System V semaphores. More specifically, learn how to get a semaphore set with semget(), change the permissions, set initialize values, etc of a semaphore set with semctl(), and perform operations on a semaphore set with semop(). Note that there is no Wiat() and Signal(), since these two functions are integrated into the single function semop(). You will see that the use of Unix semaphores is more complex than what we have learned so far. POSIX semaphores are similar with some differences.<a>

	<q>Many may have the tendency to return the value of a private variable of a monitor and test it to determine what the next step should be. For example, return the internal counter of a monitor and do something if the returned value is positive. The point is that doing this test outside of a monitor will be more efficient then doing it inside of a monitor because of less locking and unlocking (of the monitor). Whys is this usually not a wise move?

	<a>Answer


	<q>Why is calling a monitor procedure in another monitor from within a monitor not a good idea? Explain this with a convincing argument.

	<a>Answer


	<q>Why is the use of a semaphore in a monitor prohibited? Explain this with a convincing argument. Just saying that we use condition variable in a monitor is not a convincing argument. There is a much stronger reason.

	<a>Answer


	<q>There are commonly two ways of releasing threads from a condition variable. More precisely, the first method is the use of a for as follows:
	<pre>
		for (i = 0; i < n; i++)
		CV.Signal();
	</pre>

	This method uses a single thread, which is executing in a monitor, to release threads waiting on condition variable CV, where n is sufficiently large so that all blocked threads are released.
	The second method just releases one thread, and allows the released thread to release other threads. This is referred to as cascading signals.

	<pre>
		// place where threads block
		CV.Wait();
		CV.Signal();  // this one is released and release the next one

		// other work in a monitor

		CV.Signal();  // initialize a cascading release here
	</pre>

	Analyze and compare these methods under Hoare type and Mesa type monitors.

	<a>Answer


	<q>A barrier is synchronization primitive with a positive initialization value n and a method Barrier_Wait(). When a thread calls Barrier_Wait(), if it is not the n-th thread, it blocks in the barrier. When the n-th thread comes, this "last" thread releases all n-1 threads blocked in the barrier, and, as a result, all n threads, the n-th thread included, continues. Design a barrier monitor of Hoare type and method Barrier_Wait(). Which release method discussed above would you want to use? Again, analyze and compare both methods under Hoare type and Mesa type monitors. Note that before all n-1 blocked threads have been successfully released, your barrier should not allow new threads to join this activity and get "released" as part of the current batch. In other words, threads that come while the blocked threads are being released must be handled in the next batch. What would you do if the monitor is a Mesa type?

	<a>


	<q>Do the Roller-Coaster problem discussed on the class note using a Hoare monitor.

	<a>Answer


	<q>We discussed in class a solution to the philosophers problem in which each philosopher has three states: THINKING, HUNGRY and EATING. We solved this problem using a monitor. Re-do this problem using only semaphores.

	<a>Answer


	<q>Although monitors are more structured than semaphores, deadlocks may still occur if monitors are not designed carefully. Consider the following monitor of Hoare type
	<pre>
		monitor  deadlock
			condition  p, q;

			void  stop(void)
			{
				p.wait;
				......
				q.signal;
			}

			void  go(void);
			{
				......
				p.signal;
				q.wait;	 
			}
			// no initialization is required
		}
	</pre>
	Suppose we have processes P1 and P2 as follows:
	<pre>
		P1:    repeat ..... deadlock.stop; ..... until false;
		P2:    repeat ..... deadlock.go;   ..... until false;
	</pre>

	The intention of this monitor is that P1 is required to wait on condition p until P2 has completed some task, at which point it calls deadlock.go to release P1. P2 then waits until P1 has performed its task, when it signals condition q to indicate that P2 may continue.
	This program looks correct and seems can perform the indicated task. But, it has the potential to deadlock. Find this deadlock. Would the deadlock still be there is the monitor is a Mesa type?

	<a>



<w>

	<q>10-Monitors.pdf
	<q>11-Deadlock.pdf

	<q>Why is nested monitor calls (i.e., calling a monitor procedure from within a monitor) not a good practice?

	<a>Answer


	<q>Why is it a bad idea to use a semaphore in a monitor?

	<a>Answer


	<q>Design a monitor that manages a shared resource which must be accessed in a mutual exclusive way. This monitor has two methods: acquire() and release(). Before using this shared resource, a thread must call acquire(). If the resource is being used by another thread, the monitor blocks the calling thread. Otherwise, the calling thread has the permission to use. After using this shared resources, a thread calls release() so that another thread can use.

	<a>Answer


	<q>Design a barrier monitor with a Barrier_Wait() method. In last week's reading, we mentioned the cascading signals technique. If you use the Hoare type for this monitor, what is the safe way in releasing all blocked thread? Is the cascading signals more reliable? What if your monitor is of the Mesa type, would cascading signals make a difference?

	<a>Answer


	<q>Implement a semaphore using a monitor.

	<a>Answer


	<q>Recall the smokers problem discussed in class. Suppose one semaphore is used to control the table, and a semaphore is assigned to each ingredient. In this way, the agent will pick two ingredients on the table and signal the corresponding semaphores:
	<pre>
		Agent
		-----

		while (1) {
			pick two ingredients, say i and j
			signal semaphore i;
			signal semaphore j;
			wait until the table is cleared;
		}
	</pre>
	The smoker who has ingredient k and needs ingredients i and j to make a cigarette does the following:
	<pre>
		Smoker who has ingredient k
		---------------------------

		while (1) {
			wait on semaphore i;
			wait on semaphore j;
			signal semaphore table;
			make a cigarette;
		}
	</pre>

	Is this solution to the smokers problem deadlock free? If it is not deadlock free, show an execution sequence. Otherwise, prove rigorously that this solution is deadlock free.

	<a>Answer


	<q>Suppose all chopstick are placed at the center of the table, and any two of them can be used by a philosopher. Assume that requests for chopsticks are made one at a time. Design a simple rule for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers.

	<a>Answer


	<q>In the semaphore and monitor solutions to the Readers-Writers problem, a common semaphore or condition variable are used to block BOTH readers and writers. As a result, when that semaphore/condition variable is signaled, a reader or writer could be released, and there is no rule to guarantee the released process/thread is of a particular kind. We may resolve this situation with four counters: the first one counts the number of active readers, the second one counts the number of waiting readers, the third one counts the number of active writers which should be no more than 1, and the fourth one counts the number of waiting writers. Use this scheme to solve the reader-priority version with semaphores. Do the same using a monitor.

	<a>Answer


	<q>Implementing a semaphore with a monitor is an easy task. See a previous problem. How can we implement a monitor with semaphores?
	<br><br>
	Since we need mutually exclusive monitor boundary, we need a Mutex, with initial value 1, for each monitor. A process must execute Mutex.Wait() before entering the monitor and must execute Mutex.Signal() after leaving the monitor. This is exactly what MonitorBegin() and MonitorEnd() are doing in ThreadMentor.
	<br><br>
	Since a signaling process must wait until the resumed process either leaves the monitor or waits on a condition variable, we need a semaphore Next, with initial value 0, to block the suspended processes. A signaling process can use Next to block itself. To keep a record of how many suspended processes are blocked by semaphore Next , we need a counter Next_Count. Therefore, in each public monitor procedure, its code looks like the following:

	<pre>
		Mutex.Wait();
		// monitor procedure code
		if (Next_Count > 0)	// there are suspended processes
			Next.Signal();	// release one of them
		else
			Mutex.Signal();	// release an entering process
	</pre>


	Note that if there are suspended processes, this leaving process allows one of them to go but does not release the monitor. In this way, when the released process that was suspended before starts execution, it has the monitor. In other word, the leaving process passes the monitor to the released process.
	<br><br>
	To implement a condition variable CV, we need a semaphore for blocking processes CV_Sem and a variable for counting the number of waiting processes CV_Count. Both are initialized to 0.
	<br><br>
	Condition wait CV.Wait() is implement as follows:
	<pre>
		CV_Count++;           	// waiting is increased by 1
		if (Next_Count > 0)   	// if there are suspended processes
			Next.Signal();  //    release one of them to take the monitor
		else                  	// otherwise,
			Mutex.Signal(); //    release the monitor
		CV_Sem.Wait();        	// block myself
		CV_Count--;          	// if I am released, the number of wait is reduced by 1
	</pre>

	Study this implementation and make sure you understand it. Therefore, a monitor can be implemented using semaphores, and, of course, monitors and semaphores are equivalent.

	Note that this implementation is for the Hoare type monitors. Can you modify it to implement the Mesa type monitors?

	<a>Answer



<w>

	<q>12-Channels.pdf

	<q>We mentioned in class that if resources are ordered in a hierarchical way so that thread acquire resources always follow this ordering, there is no deadlock. Prove this claim rigorously.

	<a>Answer


	<q>Consider a system consisting of four identical copies resources shared by three processes, each of which needs at most two resources. Prove rigorously that the system is deadlock free.

	<a>Answer


	<q>Play with ThreadMentor's channel visualization features and learn the meaning of the following:
	<ul>
		<li>The Channel button in the History Graph window.</li>
		<li>The Channel menu button in the main window.</li>
		<li>The meaning of tags CB, CA, CS and CR.</li>
		<li>Study the two sample channel programs on ThreadMentor page.</li>
	</ul>

	<a>Answer


	<q>We mentioned in class that for the shared memory model semaphores, monitors and channels are all equivalent to each other. In other word, each of these three can be implement by the other. You were also asked as an exercise previously to implement a semaphore using a monitor, and learned (in one of our reading lists) that semaphores can be used to implement a Hoare monitor. Now you should be able to do the following:
	<ul>
		<li>Implement a semaphore using channel(s).</li>
		<li>Implement a synchronous channel using semaphores.</li>
		<li>Implement an asynchronous with bounded capacity using semaphores.</li>
	</ul>
	Consequently, you learn the equivalence among these three commonly seen synchronization mechanisms.
	<a>



<w>

	<q>13-Java-Threads.pdf
	<q>14-Ada-Tasking.pdf
	<q>15-Pthreads.pdf